{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bafaca5c-b4a8-467a-b9ee-de0a0cf500a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 ADVANCED MODELING PHASE - STACKOVERFLOW SALARY PREDICTION\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Advanced Modeling Plan - StackOverflow Salary Prediction\n",
    "# Addresses data leakage and builds robust predictive models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"🚀 ADVANCED MODELING PHASE - STACKOVERFLOW SALARY PREDICTION\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8549698-1bca-4a60-9b20-b39a1dd4c3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 STEP 1: DATA LEAKAGE DETECTION AND CLEANUP\n",
      "==================================================\n",
      "Dataset shape: (21782, 189)\n",
      "\n",
      "Potential data leakage columns detected: 3\n",
      "  ⚠️ CompTotal\n",
      "  ⚠️ AIComplex\n",
      "  ⚠️ DevType_Data_scientist_or_machine_learning_specialist\n",
      "\n",
      "Dataset after removing leakage: (21782, 186)\n",
      "Removed 3 potentially problematic features\n",
      "Removed 1 currency features\n",
      "Final clean dataset: (21782, 185)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 1: Data Leakage Detection and Removal\n",
    "# ================================================================================\n",
    "print(f\"\\n🔍 STEP 1: DATA LEAKAGE DETECTION AND CLEANUP\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load the feature engineered dataset\n",
    "df = pd.read_csv('stackoverflow_features_engineered.csv')\n",
    "TARGET_VARIABLE = 'ConvertedCompYearly'\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Identify potential data leakage features (compensation-related columns)\n",
    "LEAKAGE_KEYWORDS = [\n",
    "    'comp', 'salary', 'pay', 'wage', 'income', 'total', 'bonus', \n",
    "    'compensation', 'earning', 'remuneration'\n",
    "]\n",
    "\n",
    "potential_leakage_cols = []\n",
    "for col in df.columns:\n",
    "    if col != TARGET_VARIABLE:\n",
    "        col_lower = col.lower()\n",
    "        if any(keyword in col_lower for keyword in LEAKAGE_KEYWORDS):\n",
    "            potential_leakage_cols.append(col)\n",
    "\n",
    "print(f\"\\nPotential data leakage columns detected: {len(potential_leakage_cols)}\")\n",
    "for col in potential_leakage_cols:\n",
    "    print(f\"  ⚠️ {col}\")\n",
    "\n",
    "# Remove leakage columns\n",
    "df_clean = df.drop(columns=potential_leakage_cols)\n",
    "print(f\"\\nDataset after removing leakage: {df_clean.shape}\")\n",
    "print(f\"Removed {len(potential_leakage_cols)} potentially problematic features\")\n",
    "\n",
    "# Also remove currency features (they're too directly related to location/salary)\n",
    "currency_cols = [col for col in df_clean.columns if 'Currency' in col]\n",
    "if currency_cols:\n",
    "    df_clean = df_clean.drop(columns=currency_cols)\n",
    "    print(f\"Removed {len(currency_cols)} currency features\")\n",
    "\n",
    "print(f\"Final clean dataset: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93cd8516-e713-4c27-a082-33a76f6a7939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 STEP 2: FEATURE SELECTION AND PREPROCESSING\n",
      "==================================================\n",
      "Feature matrix: (21782, 184)\n",
      "Target variable: (21782,)\n",
      "Handling missing values in 3 columns\n",
      "Numerical features: 95\n",
      "Categorical features: 78\n",
      "Numerical features after variance threshold: 92\n",
      "Features after one-hot encoding: 58681\n",
      "Training set: 17,425 samples\n",
      "Test set: 4,357 samples\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 2: Feature Selection and Preprocessing\n",
    "# ================================================================================\n",
    "print(f\"\\n🎯 STEP 2: FEATURE SELECTION AND PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_clean.drop(columns=[TARGET_VARIABLE])\n",
    "y = df_clean[TARGET_VARIABLE]\n",
    "\n",
    "print(f\"Feature matrix: {X.shape}\")\n",
    "print(f\"Target variable: {y.shape}\")\n",
    "\n",
    "# Handle any remaining missing values\n",
    "missing_cols = X.columns[X.isnull().any()]\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"Handling missing values in {len(missing_cols)} columns\")\n",
    "    for col in missing_cols:\n",
    "        if X[col].dtype in ['float64', 'int64']:\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "        else:\n",
    "            X[col].fillna(X[col].mode().iloc[0] if len(X[col].mode()) > 0 else 'Unknown', inplace=True)\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "print(f\"Numerical features: {len(numerical_cols)}\")\n",
    "print(f\"Categorical features: {len(categorical_cols)}\")\n",
    "\n",
    "# Remove low-variance features (less than 1% variation) - only for numerical features\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "if len(numerical_cols) > 0:\n",
    "    variance_selector = VarianceThreshold(threshold=0.01)\n",
    "    X_numerical = X[numerical_cols]\n",
    "    variance_selector.fit(X_numerical)\n",
    "    selected_numerical = numerical_cols[variance_selector.get_support()]\n",
    "    print(f\"Numerical features after variance threshold: {len(selected_numerical)}\")\n",
    "else:\n",
    "    selected_numerical = []\n",
    "\n",
    "# One-hot encode categorical features\n",
    "if len(categorical_cols) > 0:\n",
    "    X_encoded = pd.get_dummies(X[categorical_cols], drop_first=True)\n",
    "    print(f\"Features after one-hot encoding: {X_encoded.shape[1]}\")\n",
    "    \n",
    "    # Combine numerical and encoded categorical features\n",
    "    if len(selected_numerical) > 0:\n",
    "        X_processed = pd.concat([X[selected_numerical], X_encoded], axis=1)\n",
    "    else:\n",
    "        X_processed = X_encoded\n",
    "else:\n",
    "    X_processed = X[selected_numerical]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "\n",
    "# Feature scaling for algorithms that need it\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936796d0-1812-4bb9-adc0-03f103ea73a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 STEP 3: BASELINE MODEL WITH CLEAN FEATURES\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 3: Baseline Model with Clean Features\n",
    "# ================================================================================\n",
    "print(f\"\\n📊 STEP 3: BASELINE MODEL WITH CLEAN FEATURES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train baseline Random Forest with clean features\n",
    "rf_baseline = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate baseline\n",
    "y_pred_baseline = rf_baseline.predict(X_test)\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "mae_baseline = mean_absolute_error(y_test, y_pred_baseline)\n",
    "rmse_baseline = np.sqrt(mean_squared_error(y_test, y_pred_baseline))\n",
    "\n",
    "print(f\"Baseline Random Forest Performance (Clean Features):\")\n",
    "print(f\"  • R² Score: {r2_baseline:.3f}\")\n",
    "print(f\"  • Mean Absolute Error: ${mae_baseline:,.0f}\")\n",
    "print(f\"  • Root Mean Square Error: ${rmse_baseline:,.0f}\")\n",
    "\n",
    "# Feature importance with clean features - use X_train columns instead of X\n",
    "# This ensures the feature names match the model's training data\n",
    "feature_importance_clean = pd.DataFrame({\n",
    "    'feature': X_train.columns,  # Changed from X.columns to X_train.columns\n",
    "    'importance': rf_baseline.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 15 features (without data leakage):\")\n",
    "for i, (_, row) in enumerate(feature_importance_clean.head(15).iterrows(), 1):\n",
    "    print(f\"  {i:2d}. {row['feature']}: {row['importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67965a7a-b32c-458a-b4ca-5b1f791a3e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 STEP 4: ADVANCED MODEL TRAINING AND COMPARISON (FAST VERSION)\n",
      "==================================================\n",
      "⏱️ Estimated execution time: 15-20 minutes\n",
      "📊 Parameter combinations reduced for speed while maintaining quality\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Define models to compare - REDUCED PARAMETER GRIDS FOR SPEED\u001b[39;00m\n\u001b[1;32m     11\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[0;32m---> 13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m],          \u001b[38;5;66;03m# Reduced from [100, 200, 300]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],             \u001b[38;5;66;03m# Reduced from [10, 20, None]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m],         \u001b[38;5;66;03m# Reduced from [2, 5, 10]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m]          \u001b[38;5;66;03m# Reduced from [1, 2, 4]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         },\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaled\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     },\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGradient Boosting\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: GradientBoostingRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m],          \u001b[38;5;66;03m# Reduced from [100, 200, 300]\u001b[39;00m\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m],                 \u001b[38;5;66;03m# Reduced from [3, 5, 7]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.15\u001b[39m],       \u001b[38;5;66;03m# Reduced from [0.05, 0.1, 0.2]\u001b[39;00m\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubsample\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m1.0\u001b[39m]             \u001b[38;5;66;03m# Reduced from [0.8, 0.9, 1.0]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         },\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaled\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     },\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#    'Ridge Regression': {\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#        'model': Ridge(random_state=42),\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#        'params': {\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#            'alpha': [1.0, 10.0, 100.0]         # Reduced from [0.1, 1.0, 10.0, 100.0, 1000.0]\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#        },\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#        'scaled': True\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#    },\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#    'Lasso Regression': {\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#        'model': Lasso(random_state=42, max_iter=2000),\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#        'params': {\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#            'alpha': [1.0, 10.0, 100.0]         # Reduced from [0.1, 1.0, 10.0, 100.0, 1000.0]\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#        },\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#        'scaled': True\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#    },\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#    'Extra Trees': {\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#        'model': ExtraTreesRegressor(random_state=42, n_jobs=-1),\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#        'params': {\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#            'n_estimators': [100, 200],          # Reduced from [100, 200]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#            'max_depth': [10, None],             # Reduced from [10, 20, None]\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#            'min_samples_split': [2, 5]         # Reduced from [2, 5, 10]\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#        },\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#        'scaled': False\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#    }\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Removed Elastic Net for speed - you can add it back if you want\u001b[39;00m\n\u001b[1;32m     56\u001b[0m }\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Train and evaluate all models\u001b[39;00m\n\u001b[1;32m     59\u001b[0m model_results \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 4: Advanced Model Training and Comparison (FAST VERSION - 15-20 minutes)\n",
    "# ================================================================================\n",
    "print(f\"\\n🤖 STEP 4: ADVANCED MODEL TRAINING AND COMPARISON (FAST VERSION)\")\n",
    "print(\"=\"*50)\n",
    "print(\"⏱️ Estimated execution time: 15-20 minutes\")\n",
    "print(\"📊 Parameter combinations reduced for speed while maintaining quality\")\n",
    "print()\n",
    "\n",
    "# Define models to compare - REDUCED PARAMETER GRIDS FOR SPEED\n",
    "models = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],          # Reduced from [100, 200, 300]\n",
    "            'max_depth': [10, None],             # Reduced from [10, 20, None]\n",
    "            'min_samples_split': [2, 5],         # Reduced from [2, 5, 10]\n",
    "            'min_samples_leaf': [1, 2]          # Reduced from [1, 2, 4]\n",
    "        },\n",
    "        'scaled': False\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],          # Reduced from [100, 200, 300]\n",
    "            'max_depth': [3, 5],                 # Reduced from [3, 5, 7]\n",
    "            'learning_rate': [0.1, 0.15],       # Reduced from [0.05, 0.1, 0.2]\n",
    "            'subsample': [0.8, 1.0]             # Reduced from [0.8, 0.9, 1.0]\n",
    "        },\n",
    "        'scaled': False\n",
    "    },\n",
    "#    'Ridge Regression': {\n",
    "#        'model': Ridge(random_state=42),\n",
    "#        'params': {\n",
    "#            'alpha': [1.0, 10.0, 100.0]         # Reduced from [0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "#        },\n",
    "#        'scaled': True\n",
    "#    },\n",
    "#    'Lasso Regression': {\n",
    "#        'model': Lasso(random_state=42, max_iter=2000),\n",
    "#        'params': {\n",
    "#            'alpha': [1.0, 10.0, 100.0]         # Reduced from [0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "#        },\n",
    "#        'scaled': True\n",
    "#    },\n",
    "#    'Extra Trees': {\n",
    "#        'model': ExtraTreesRegressor(random_state=42, n_jobs=-1),\n",
    "#        'params': {\n",
    "#            'n_estimators': [100, 200],          # Reduced from [100, 200]\n",
    "#            'max_depth': [10, None],             # Reduced from [10, 20, None]\n",
    "#            'min_samples_split': [2, 5]         # Reduced from [2, 5, 10]\n",
    "#        },\n",
    "#        'scaled': False\n",
    "#    }\n",
    "    # Removed Elastic Net for speed - you can add it back if you want\n",
    "}\n",
    "\n",
    "# Train and evaluate all models\n",
    "model_results = {}\n",
    "\n",
    "for model_name, model_config in models.items():\n",
    "    print(f\"\\n🔧 Training {model_name}...\")\n",
    "    \n",
    "    # Choose scaled or unscaled features\n",
    "    X_train_use = X_train_scaled if model_config['scaled'] else X_train\n",
    "    X_test_use = X_test_scaled if model_config['scaled'] else X_test\n",
    "    \n",
    "    # Hyperparameter tuning with GridSearch (reduced CV for speed)\n",
    "    grid_search = GridSearchCV(\n",
    "        model_config['model'],\n",
    "        model_config['params'],\n",
    "        cv=3,              # Reduced from 5 to 3 for speed\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_use, y_train)\n",
    "    \n",
    "    # Best model evaluation\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test_use)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    # Cross-validation score (reduced CV for speed)\n",
    "    cv_scores = cross_val_score(best_model, X_train_use, y_train, cv=3, scoring='r2')\n",
    "    \n",
    "    model_results[model_name] = {\n",
    "        'model': best_model,\n",
    "        'r2_score': r2,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'scaled': model_config['scaled']\n",
    "    }\n",
    "    \n",
    "    print(f\"  ✅ {model_name} - R²: {r2:.3f}, MAE: ${mae:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c684840-b9ab-4509-b490-0b169a117591",
   "metadata": {},
   "source": [
    "# Train and evaluate all models\n",
    "model_results = {}\n",
    "\n",
    "for model_name, model_config in models.items():\n",
    "    print(f\"\\n🔧 Training {model_name}...\")\n",
    "    \n",
    "    # Choose scaled or unscaled features\n",
    "    X_train_use = X_train_scaled if model_config['scaled'] else X_train\n",
    "    X_test_use = X_test_scaled if model_config['scaled'] else X_test\n",
    "    \n",
    "    # Hyperparameter tuning with GridSearch\n",
    "    grid_search = GridSearchCV(\n",
    "        model_config['model'],\n",
    "        model_config['params'],\n",
    "        cv=5,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_use, y_train)\n",
    "    \n",
    "    # Best model evaluation\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test_use)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(best_model, X_train_use, y_train, cv=5, scoring='r2')\n",
    "    \n",
    "    model_results[model_name] = {\n",
    "        'model': best_model,\n",
    "        'r2_score': r2,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'scaled': model_config['scaled']\n",
    "    }\n",
    "    \n",
    "    print(f\"  ✅ {model_name} - R²: {r2:.3f}, MAE: ${mae:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0eaf1d3-a7d3-4230-987c-f58a4ee9752f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏆 STEP 5: MODEL COMPARISON AND SELECTION\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create comparison DataFrame\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m comparison_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m      9\u001b[0m     model_name: {\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR² Score\u001b[39m\u001b[38;5;124m'\u001b[39m: results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2_score\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE ($)\u001b[39m\u001b[38;5;124m'\u001b[39m: results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE ($)\u001b[39m\u001b[38;5;124m'\u001b[39m: results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCV Mean\u001b[39m\u001b[38;5;124m'\u001b[39m: results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv_mean\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCV Std\u001b[39m\u001b[38;5;124m'\u001b[39m: results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv_std\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m     }\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model_name, results \u001b[38;5;129;01min\u001b[39;00m model_results\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     17\u001b[0m })\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Performance Comparison:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(comparison_df\u001b[38;5;241m.\u001b[39mT)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 5: Model Comparison and Selection\n",
    "# ================================================================================\n",
    "print(f\"\\n🏆 STEP 5: MODEL COMPARISON AND SELECTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    model_name: {\n",
    "        'R² Score': results['r2_score'],\n",
    "        'MAE ($)': results['mae'],\n",
    "        'RMSE ($)': results['rmse'],\n",
    "        'CV Mean': results['cv_mean'],\n",
    "        'CV Std': results['cv_std']\n",
    "    }\n",
    "    for model_name, results in model_results.items()\n",
    "}).round(3)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(comparison_df.T)\n",
    "\n",
    "# Select best model\n",
    "best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['r2_score'])\n",
    "best_model = model_results[best_model_name]['model']\n",
    "best_r2 = model_results[best_model_name]['r2_score']\n",
    "\n",
    "print(f\"\\n🥇 Best Model: {best_model_name}\")\n",
    "print(f\"   R² Score: {best_r2:.3f}\")\n",
    "print(f\"   MAE: ${model_results[best_model_name]['mae']:,.0f}\")\n",
    "print(f\"   Best Parameters: {model_results[best_model_name]['best_params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e33262c4-7f09-4f31-a49b-f8c091dc9e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 STEP 6: MODEL EVALUATION AND DIAGNOSTICS\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Use best model for detailed evaluation\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m best_scaled \u001b[38;5;241m=\u001b[39m model_results[best_model_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaled\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m X_test_final \u001b[38;5;241m=\u001b[39m X_test_scaled \u001b[38;5;28;01mif\u001b[39;00m best_scaled \u001b[38;5;28;01melse\u001b[39;00m X_test\n\u001b[1;32m     10\u001b[0m y_pred_final \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_test_final)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_results' is not defined"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 6: Model Evaluation and Diagnostics\n",
    "# ================================================================================\n",
    "print(f\"\\n📈 STEP 6: MODEL EVALUATION AND DIAGNOSTICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use best model for detailed evaluation\n",
    "best_scaled = model_results[best_model_name]['scaled']\n",
    "X_test_final = X_test_scaled if best_scaled else X_test\n",
    "y_pred_final = best_model.predict(X_test_final)\n",
    "\n",
    "# Residual analysis\n",
    "residuals = y_test - y_pred_final\n",
    "\n",
    "# Create evaluation plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Actual vs Predicted\n",
    "axes[0,0].scatter(y_test, y_pred_final, alpha=0.6)\n",
    "axes[0,0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0,0].set_xlabel('Actual Salary')\n",
    "axes[0,0].set_ylabel('Predicted Salary')\n",
    "axes[0,0].set_title(f'Actual vs Predicted\\n{best_model_name} (R² = {best_r2:.3f})')\n",
    "\n",
    "# 2. Residuals vs Predicted\n",
    "axes[0,1].scatter(y_pred_final, residuals, alpha=0.6)\n",
    "axes[0,1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0,1].set_xlabel('Predicted Salary')\n",
    "axes[0,1].set_ylabel('Residuals')\n",
    "axes[0,1].set_title('Residuals vs Predicted')\n",
    "\n",
    "# 3. Residual histogram\n",
    "axes[0,2].hist(residuals, bins=30, alpha=0.7)\n",
    "axes[0,2].set_xlabel('Residuals')\n",
    "axes[0,2].set_ylabel('Frequency')\n",
    "axes[0,2].set_title('Residual Distribution')\n",
    "\n",
    "# 4. Feature importance (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    top_features = feature_importance_clean.head(15)\n",
    "    axes[1,0].barh(range(len(top_features)), top_features['importance'])\n",
    "    axes[1,0].set_yticks(range(len(top_features)))\n",
    "    axes[1,0].set_yticklabels(top_features['feature'], fontsize=8)\n",
    "    axes[1,0].set_xlabel('Importance')\n",
    "    axes[1,0].set_title('Top 15 Feature Importances')\n",
    "\n",
    "# 5. Learning curve\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_model, X_train_scaled if best_scaled else X_train, y_train, \n",
    "    cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "axes[1,1].plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training Score')\n",
    "axes[1,1].plot(train_sizes, np.mean(val_scores, axis=1), 'o-', label='Validation Score')\n",
    "axes[1,1].set_xlabel('Training Set Size')\n",
    "axes[1,1].set_ylabel('R² Score')\n",
    "axes[1,1].set_title('Learning Curve')\n",
    "axes[1,1].legend()\n",
    "\n",
    "# 6. Prediction error distribution\n",
    "prediction_errors = np.abs(residuals)\n",
    "axes[1,2].hist(prediction_errors, bins=30, alpha=0.7)\n",
    "axes[1,2].axvline(prediction_errors.mean(), color='r', linestyle='--', \n",
    "                 label=f'Mean Error: ${prediction_errors.mean():,.0f}')\n",
    "axes[1,2].set_xlabel('Absolute Prediction Error ($)')\n",
    "axes[1,2].set_ylabel('Frequency')\n",
    "axes[1,2].set_title('Prediction Error Distribution')\n",
    "axes[1,2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8c8b298-8429-4e46-94e5-b39a430b2d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💡 STEP 7: MODEL INTERPRETATION AND INSIGHTS\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Feature importance analysis (for tree-based models)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(best_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_importances_\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🌟 TOP FEATURE INSIGHTS (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     top_10_features \u001b[38;5;241m=\u001b[39m feature_importance_clean\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 7: Model Interpretation and Insights\n",
    "# ================================================================================\n",
    "print(f\"\\n💡 STEP 7: MODEL INTERPRETATION AND INSIGHTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Feature importance analysis (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    print(f\"🌟 TOP FEATURE INSIGHTS ({best_model_name}):\")\n",
    "    \n",
    "    top_10_features = feature_importance_clean.head(10)\n",
    "    total_importance = top_10_features['importance'].sum()\n",
    "    \n",
    "    for i, (_, row) in enumerate(top_10_features.iterrows(), 1):\n",
    "        importance_pct = (row['importance'] / total_importance) * 100\n",
    "        print(f\"  {i:2d}. {row['feature']}: {row['importance']:.3f} ({importance_pct:.1f}% of top 10)\")\n",
    "\n",
    "# Performance summary\n",
    "print(f\"\\n📊 FINAL MODEL PERFORMANCE SUMMARY:\")\n",
    "print(f\"  • Model: {best_model_name}\")\n",
    "print(f\"  • R² Score: {best_r2:.3f} (explains {best_r2*100:.1f}% of salary variance)\")\n",
    "print(f\"  • Mean Absolute Error: ${model_results[best_model_name]['mae']:,.0f}\")\n",
    "print(f\"  • Cross-validation: {model_results[best_model_name]['cv_mean']:.3f} ± {model_results[best_model_name]['cv_std']:.3f}\")\n",
    "\n",
    "# Business interpretation\n",
    "median_salary = y_test.median()\n",
    "mae_percentage = (model_results[best_model_name]['mae'] / median_salary) * 100\n",
    "\n",
    "print(f\"\\n💼 BUSINESS INTERPRETATION:\")\n",
    "print(f\"  • Model predicts salary within ${model_results[best_model_name]['mae']:,.0f} on average\")\n",
    "print(f\"  • This represents {mae_percentage:.1f}% of median salary (${median_salary:,.0f})\")\n",
    "print(f\"  • Model explains {best_r2*100:.1f}% of salary variation\")\n",
    "\n",
    "if best_r2 >= 0.70:\n",
    "    quality = \"Excellent\"\n",
    "elif best_r2 >= 0.60:\n",
    "    quality = \"Good\"\n",
    "elif best_r2 >= 0.50:\n",
    "    quality = \"Moderate\"\n",
    "else:\n",
    "    quality = \"Poor\"\n",
    "\n",
    "print(f\"  • Model quality: {quality} ({best_r2:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93611c81-43da-416c-af10-f7e074597a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 STEP 8: SAVING FINAL MODEL AND RESULTS\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Save the best model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(best_model, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(scaler, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_scaler.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Save model comparison results\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 8: Save Final Model and Results\n",
    "# ================================================================================\n",
    "print(f\"\\n💾 STEP 8: SAVING FINAL MODEL AND RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save the best model\n",
    "import joblib\n",
    "joblib.dump(best_model, f'best_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl')\n",
    "joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "\n",
    "# Save model comparison results\n",
    "comparison_df.to_csv('model_comparison_results.csv')\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance_clean.to_csv('final_feature_importance.csv', index=False)\n",
    "\n",
    "# Save final dataset splits\n",
    "np.save('X_train_final.npy', X_train_scaled if best_scaled else X_train)\n",
    "np.save('X_test_final.npy', X_test_scaled if best_scaled else X_test)\n",
    "np.save('y_train_final.npy', y_train.values)\n",
    "np.save('y_test_final.npy', y_test.values)\n",
    "\n",
    "# Save model summary\n",
    "model_summary = {\n",
    "    'best_model': best_model_name,\n",
    "    'r2_score': float(best_r2),\n",
    "    'mae': float(model_results[best_model_name]['mae']),\n",
    "    'rmse': float(model_results[best_model_name]['rmse']),\n",
    "    'cv_mean': float(model_results[best_model_name]['cv_mean']),\n",
    "    'cv_std': float(model_results[best_model_name]['cv_std']),\n",
    "    'best_params': model_results[best_model_name]['best_params'],\n",
    "    'features_used': len(X.columns),\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'top_5_features': feature_importance_clean.head(5)['feature'].tolist()\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('model_summary.json', 'w') as f:\n",
    "    json.dump(model_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✅ Files saved:\")\n",
    "print(f\"  • best_model_{best_model_name.lower().replace(' ', '_')}.pkl\")\n",
    "print(f\"  • feature_scaler.pkl\")\n",
    "print(f\"  • model_comparison_results.csv\")\n",
    "print(f\"  • final_feature_importance.csv\")\n",
    "print(f\"  • model_summary.json\")\n",
    "\n",
    "print(f\"\\n🎯 READY FOR CREATIVE SCENARIO PREDICTIONS!\")\n",
    "print(f\"Next steps:\")\n",
    "print(f\"  1. Creative scenario predictions\")\n",
    "print(f\"  2. Blog post writing with insights\")\n",
    "print(f\"  3. GitHub repository setup\")\n",
    "print(f\"  4. Final presentation preparation\")\n",
    "\n",
    "print(f\"\\n✅ ADVANCED MODELING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40232bc0-e214-4f10-9e64-06d97b266bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
