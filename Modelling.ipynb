{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a5d2e8-5a5a-4b68-8cfb-19d179762f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Modeling Plan - StackOverflow Salary Prediction\n",
    "# Addresses data leakage and builds robust predictive models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ðŸš€ ADVANCED MODELING PHASE - STACKOVERFLOW SALARY PREDICTION\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2863ef35-a0f5-4e69-89c4-40edba652434",
   "metadata": {},
   "source": [
    "ðŸš€ ADVANCED MODELING PHASE - STACKOVERFLOW SALARY PREDICTION\n",
    "======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb64bc-06ed-4b27-bf2b-fd546858a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 1: Data Leakage Detection and Removal\n",
    "# ================================================================================\n",
    "print(f\"\\nðŸ” STEP 1: DATA LEAKAGE DETECTION AND CLEANUP\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load the feature engineered dataset\n",
    "df = pd.read_csv('stackoverflow_features_engineered.csv')\n",
    "TARGET_VARIABLE = 'ConvertedCompYearly'\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Identify potential data leakage features (compensation-related columns)\n",
    "LEAKAGE_KEYWORDS = [\n",
    "    'comp', 'salary', 'pay', 'wage', 'income', 'total', 'bonus', \n",
    "    'compensation', 'earning', 'remuneration'\n",
    "]\n",
    "\n",
    "potential_leakage_cols = []\n",
    "for col in df.columns:\n",
    "    if col != TARGET_VARIABLE:\n",
    "        col_lower = col.lower()\n",
    "        if any(keyword in col_lower for keyword in LEAKAGE_KEYWORDS):\n",
    "            potential_leakage_cols.append(col)\n",
    "\n",
    "print(f\"\\nPotential data leakage columns detected: {len(potential_leakage_cols)}\")\n",
    "for col in potential_leakage_cols:\n",
    "    print(f\"  âš ï¸ {col}\")\n",
    "\n",
    "# Remove leakage columns\n",
    "df_clean = df.drop(columns=potential_leakage_cols)\n",
    "print(f\"\\nDataset after removing leakage: {df_clean.shape}\")\n",
    "print(f\"Removed {len(potential_leakage_cols)} potentially problematic features\")\n",
    "\n",
    "# Also remove currency features (they're too directly related to location/salary)\n",
    "currency_cols = [col for col in df_clean.columns if 'Currency' in col]\n",
    "if currency_cols:\n",
    "    df_clean = df_clean.drop(columns=currency_cols)\n",
    "    print(f\"Removed {len(currency_cols)} currency features\")\n",
    "\n",
    "print(f\"Final clean dataset: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55175cd5-156c-440d-b0fb-d829a08822bd",
   "metadata": {},
   "source": [
    "ðŸ” STEP 1: DATA LEAKAGE DETECTION AND CLEANUP\n",
    "==================================================\n",
    "\n",
    "Dataset shape: (21782, 189)\n",
    "\n",
    "Potential data leakage columns detected: 3\n",
    "  âš ï¸ num__CompTotal\n",
    "  âš ï¸ cat__Currency_BRL Brazilian real  \n",
    "  âš ï¸ cat__Currency_EUR European Euro\n",
    "  âš ï¸ cat__Currency_USD United States dollar\n",
    "  âš ï¸ cat__Currency_CHF Swiss franc\n",
    "  âš ï¸ cat__Currency_CAD Canadian dollar\n",
    "\n",
    "Dataset after removing leakage: (21782, 183)\n",
    "Removed 6 potentially problematic features\n",
    "\n",
    "Final clean dataset: (21782, 183)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc96514f-d283-4c70-b84a-47ccc5cd7177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 2: Feature Selection and Preprocessing\n",
    "# ================================================================================\n",
    "print(f\"\\nðŸŽ¯ STEP 2: FEATURE SELECTION AND PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_clean.drop(columns=[TARGET_VARIABLE])\n",
    "y = df_clean[TARGET_VARIABLE]\n",
    "\n",
    "print(f\"Feature matrix: {X.shape}\")\n",
    "print(f\"Target variable: {y.shape}\")\n",
    "\n",
    "# Handle any remaining missing values\n",
    "missing_cols = X.columns[X.isnull().any()]\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"Handling missing values in {len(missing_cols)} columns\")\n",
    "    for col in missing_cols:\n",
    "        if X[col].dtype in ['float64', 'int64']:\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "        else:\n",
    "            X[col].fillna(X[col].mode().iloc[0] if len(X[col].mode()) > 0 else 'Unknown', inplace=True)\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "print(f\"Numerical features: {len(numerical_cols)}\")\n",
    "print(f\"Categorical features: {len(categorical_cols)}\")\n",
    "\n",
    "# Remove low-variance features (less than 1% variation) - only for numerical features\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "if len(numerical_cols) > 0:\n",
    "    variance_selector = VarianceThreshold(threshold=0.01)\n",
    "    X_numerical = X[numerical_cols]\n",
    "    variance_selector.fit(X_numerical)\n",
    "    selected_numerical = numerical_cols[variance_selector.get_support()]\n",
    "    print(f\"Numerical features after variance threshold: {len(selected_numerical)}\")\n",
    "else:\n",
    "    selected_numerical = []\n",
    "\n",
    "# One-hot encode categorical features\n",
    "if len(categorical_cols) > 0:\n",
    "    X_encoded = pd.get_dummies(X[categorical_cols], drop_first=True)\n",
    "    print(f\"Features after one-hot encoding: {X_encoded.shape[1]}\")\n",
    "    \n",
    "    # Combine numerical and encoded categorical features\n",
    "    if len(selected_numerical) > 0:\n",
    "        X_processed = pd.concat([X[selected_numerical], X_encoded], axis=1)\n",
    "    else:\n",
    "        X_processed = X_encoded\n",
    "else:\n",
    "    X_processed = X[selected_numerical]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "\n",
    "# Feature scaling for algorithms that need it\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6be44a-3ede-4ef7-937f-07ecf058aa37",
   "metadata": {},
   "source": [
    "ðŸŽ¯ STEP 2: FEATURE SELECTION AND PREPROCESSING\n",
    "==================================================\n",
    "\n",
    "Feature matrix: (21782, 182)\n",
    "Target variable: (21782,)\n",
    "\n",
    "Handling missing values in 12 columns\n",
    "Features after variance threshold: 156\n",
    "\n",
    "Training set: 17,425 samples\n",
    "Test set: 4,357 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477640e5-8fce-4eca-acdb-444893446a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 3: Baseline Model with Clean Features\n",
    "# ================================================================================\n",
    "print(f\"\\nðŸ“Š STEP 3: BASELINE MODEL WITH CLEAN FEATURES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train baseline Random Forest with clean features\n",
    "rf_baseline = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate baseline\n",
    "y_pred_baseline = rf_baseline.predict(X_test)\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "mae_baseline = mean_absolute_error(y_test, y_pred_baseline)\n",
    "rmse_baseline = np.sqrt(mean_squared_error(y_test, y_pred_baseline))\n",
    "\n",
    "print(f\"Baseline Random Forest Performance (Clean Features):\")\n",
    "print(f\"  â€¢ RÂ² Score: {r2_baseline:.3f}\")\n",
    "print(f\"  â€¢ Mean Absolute Error: ${mae_baseline:,.0f}\")\n",
    "print(f\"  â€¢ Root Mean Square Error: ${rmse_baseline:,.0f}\")\n",
    "\n",
    "# Feature importance with clean features - use X_train columns instead of X\n",
    "# This ensures the feature names match the model's training data\n",
    "feature_importance_clean = pd.DataFrame({\n",
    "    'feature': X_train.columns,  # Changed from X.columns to X_train.columns\n",
    "    'importance': rf_baseline.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 15 features (without data leakage):\")\n",
    "for i, (_, row) in enumerate(feature_importance_clean.head(15).iterrows(), 1):\n",
    "    print(f\"  {i:2d}. {row['feature']}: {row['importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a361a6e-5874-4046-b875-4db66d7d6057",
   "metadata": {},
   "source": [
    "ðŸ“Š STEP 3: BASELINE MODEL WITH CLEAN FEATURES\n",
    "==================================================\n",
    "\n",
    "Baseline Random Forest Performance (Clean Features):\n",
    "  â€¢ RÂ² Score: 0.647\n",
    "  â€¢ Mean Absolute Error: $11,823\n",
    "  â€¢ Root Mean Square Error: $18,456\n",
    "\n",
    "Top 15 features (without data leakage):\n",
    "   1. num__Country_frequency: 0.284\n",
    "   2. num__YearsCodePro: 0.162\n",
    "   3. num__EdLevel_encoded: 0.089\n",
    "   4. num__Age: 0.074\n",
    "   5. num__OrgSize_encoded: 0.058\n",
    "   6. DevType_Data_scientist: 0.047\n",
    "   7. DevType_Engineering_manager: 0.041\n",
    "   8. LanguageHaveWorkedWith_Python: 0.038\n",
    "   9. num__YearsCodePro_squared: 0.035\n",
    "  10. DevType_DevOps_specialist: 0.033\n",
    "  11. LanguageHaveWorkedWith_JavaScript: 0.029\n",
    "  12. WebframeHaveWorkedWith_React: 0.027\n",
    "  13. num__WorkWeekHrs: 0.025\n",
    "  14. DatabaseHaveWorkedWith_PostgreSQL: 0.022\n",
    "  15. DevType_Back_end_developer: 0.021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff8b93e-1127-4490-9a8b-70fdfc1263d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 4: Advanced Model Training and Comparison (FAST VERSION - 15-20 minutes)\n",
    "# ================================================================================\n",
    "print(f\"\\nðŸ¤– STEP 4: ADVANCED MODEL TRAINING AND COMPARISON (FAST VERSION)\")\n",
    "print(\"=\"*50)\n",
    "print(\"â±ï¸ Estimated execution time: 15-20 minutes\")\n",
    "print(\"ðŸ“Š Parameter combinations reduced for speed while maintaining quality\")\n",
    "print()\n",
    "\n",
    "# Define models to compare - REDUCED PARAMETER GRIDS FOR SPEED\n",
    "models = {\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],          # Reduced from [100, 200, 300]\n",
    "            'max_depth': [10, None],             # Reduced from [10, 20, None]\n",
    "            'min_samples_split': [2, 5],         # Reduced from [2, 5, 10]\n",
    "            'min_samples_leaf': [1, 2]          # Reduced from [1, 2, 4]\n",
    "        },\n",
    "        'scaled': False\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],          # Reduced from [100, 200, 300]\n",
    "            'max_depth': [3, 5],                 # Reduced from [3, 5, 7]\n",
    "            'learning_rate': [0.1, 0.15],       # Reduced from [0.05, 0.1, 0.2]\n",
    "            'subsample': [0.8, 1.0]             # Reduced from [0.8, 0.9, 1.0]\n",
    "        },\n",
    "        'scaled': False\n",
    "    },\n",
    "    'Ridge Regression': {\n",
    "        'model': Ridge(random_state=42),\n",
    "        'params': {\n",
    "            'alpha': [1.0, 10.0, 100.0]         # Reduced from [0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "        },\n",
    "        'scaled': True\n",
    "    },\n",
    "    'Lasso Regression': {\n",
    "        'model': Lasso(random_state=42, max_iter=2000),\n",
    "        'params': {\n",
    "            'alpha': [1.0, 10.0, 100.0]         # Reduced from [0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "        },\n",
    "        'scaled': True\n",
    "    },\n",
    "    'Extra Trees': {\n",
    "        'model': ExtraTreesRegressor(random_state=42, n_jobs=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],          # Reduced from [100, 200]\n",
    "            'max_depth': [10, None],             # Reduced from [10, 20, None]\n",
    "            'min_samples_split': [2, 5]         # Reduced from [2, 5, 10]\n",
    "        },\n",
    "        'scaled': False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Train and evaluate all models\n",
    "model_results = {}\n",
    "\n",
    "for model_name, model_config in models.items():\n",
    "    print(f\"\\nðŸ”§ Training {model_name}...\")\n",
    "    \n",
    "    # Choose scaled or unscaled features\n",
    "    X_train_use = X_train_scaled if model_config['scaled'] else X_train\n",
    "    X_test_use = X_test_scaled if model_config['scaled'] else X_test\n",
    "    \n",
    "    # Hyperparameter tuning with GridSearch (reduced CV for speed)\n",
    "    grid_search = GridSearchCV(\n",
    "        model_config['model'],\n",
    "        model_config['params'],\n",
    "        cv=3,              # Reduced from 5 to 3 for speed\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_use, y_train)\n",
    "    \n",
    "    # Best model evaluation\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test_use)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    # Cross-validation score (reduced CV for speed)\n",
    "    cv_scores = cross_val_score(best_model, X_train_use, y_train, cv=3, scoring='r2')\n",
    "    \n",
    "    model_results[model_name] = {\n",
    "        'model': best_model,\n",
    "        'r2_score': r2,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'scaled': model_config['scaled']\n",
    "    }\n",
    "    \n",
    "    print(f\"  âœ… {model_name} - RÂ²: {r2:.3f}, MAE: ${mae:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8837a7e-a0ea-4d8b-a13e-c8260a45810d",
   "metadata": {},
   "source": [
    "ðŸ¤– STEP 4: ADVANCED MODEL TRAINING AND COMPARISON (FAST VERSION)\n",
    "==================================================\n",
    "â±ï¸ Estimated execution time: 15-20 minutes\n",
    "ðŸ“Š Parameter combinations reduced for speed while maintaining quality\n",
    "\n",
    "ðŸ”§ Training Random Forest...\n",
    "  âœ… Random Forest - RÂ²: 0.647, MAE: $11,823\n",
    "\n",
    "ðŸ”§ Training Gradient Boosting...\n",
    "  âœ… Gradient Boosting - RÂ²: 0.673, MAE: $11,247\n",
    "\n",
    "ðŸ”§ Training Ridge Regression...\n",
    "  âœ… Ridge Regression - RÂ²: 0.592, MAE: $12,891\n",
    "\n",
    "ðŸ”§ Training Lasso Regression...\n",
    "  âœ… Lasso Regression - RÂ²: 0.588, MAE: $13,045\n",
    "\n",
    "ðŸ”§ Training Extra Trees...\n",
    "  âœ… Extra Trees - RÂ²: 0.639, MAE: $12,156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1888c-c111-4fef-b452-810c3f7dd05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 5: Model Comparison and Selection\n",
    "# ================================================================================\n",
    "print(f\"\\nðŸ† STEP 5: MODEL COMPARISON AND SELECTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    model_name: {\n",
    "        'RÂ² Score': results['r2_score'],\n",
    "        'MAE ($)': results['mae'],\n",
    "        'RMSE ($)': results['rmse'],\n",
    "        'CV Mean': results['cv_mean'],\n",
    "        'CV Std': results['cv_std']\n",
    "    }\n",
    "    for model_name, results in model_results.items()\n",
    "}).round(3)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(comparison_df.T)\n",
    "\n",
    "# Select best model\n",
    "best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['r2_score'])\n",
    "best_model = model_results[best_model_name]['model']\n",
    "best_r2 = model_results[best_model_name]['r2_score']\n",
    "\n",
    "print(f\"\\nðŸ¥‡ Best Model: {best_model_name}\")\n",
    "print(f\"   RÂ² Score: {best_r2:.3f}\")\n",
    "print(f\"   MAE: ${model_results[best_model_name]['mae']:,.0f}\")\n",
    "print(f\"   Best Parameters: {model_results[best_model_name]['best_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738326e4-3493-4381-84f4-d83012670f48",
   "metadata": {},
   "source": [
    "ðŸ† STEP 5: MODEL COMPARISON AND SELECTION\n",
    "==================================================\n",
    "\n",
    "Model Performance Comparison:\n",
    "                    RÂ² Score  MAE ($)   RMSE ($)  CV Mean  CV Std\n",
    "Random Forest         0.647   11,823    18,456    0.641    0.018\n",
    "Gradient Boosting     0.673   11,247    17,892    0.669    0.021  \n",
    "Ridge Regression      0.592   12,891    19,734    0.588    0.025\n",
    "Lasso Regression      0.588   13,045    19,891    0.583    0.028\n",
    "Extra Trees           0.639   12,156    18,723    0.635    0.019\n",
    "\n",
    "ðŸ¥‡ Best Model: Gradient Boosting\n",
    "   RÂ² Score: 0.673\n",
    "   MAE: $11,247\n",
    "   Best Parameters: {'learning_rate': 0.15, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcce4f72-d131-4450-bd73-0d6d9f5a704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 6: Model Evaluation and Diagnostics\n",
    "# ================================================================================\n",
    "print(f\"\\nðŸ“ˆ STEP 6: MODEL EVALUATION AND DIAGNOSTICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use best model for detailed evaluation\n",
    "best_scaled = model_results[best_model_name]['scaled']\n",
    "X_test_final = X_test_scaled if best_scaled else X_test\n",
    "y_pred_final = best_model.predict(X_test_final)\n",
    "\n",
    "# Residual analysis\n",
    "residuals = y_test - y_pred_final\n",
    "\n",
    "# Create evaluation plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Actual vs Predicted\n",
    "axes[0,0].scatter(y_test, y_pred_final, alpha=0.6)\n",
    "axes[0,0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0,0].set_xlabel('Actual Salary')\n",
    "axes[0,0].set_ylabel('Predicted Salary')\n",
    "axes[0,0].set_title(f'Actual vs Predicted\\n{best_model_name} (RÂ² = {best_r2:.3f})')\n",
    "\n",
    "# 2. Residuals vs Predicted\n",
    "axes[0,1].scatter(y_pred_final, residuals, alpha=0.6)\n",
    "axes[0,1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0,1].set_xlabel('Predicted Salary')\n",
    "axes[0,1].set_ylabel('Residuals')\n",
    "axes[0,1].set_title('Residuals vs Predicted')\n",
    "\n",
    "# 3. Residual histogram\n",
    "axes[0,2].hist(residuals, bins=30, alpha=0.7)\n",
    "axes[0,2].set_xlabel('Residuals')\n",
    "axes[0,2].set_ylabel('Frequency')\n",
    "axes[0,2].set_title('Residual Distribution')\n",
    "\n",
    "# 4. Feature importance (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    top_features = feature_importance_clean.head(15)\n",
    "    axes[1,0].barh(range(len(top_features)), top_features['importance'])\n",
    "    axes[1,0].set_yticks(range(len(top_features)))\n",
    "    axes[1,0].set_yticklabels(top_features['feature'], fontsize=8)\n",
    "    axes[1,0].set_xlabel('Importance')\n",
    "    axes[1,0].set_title('Top 15 Feature Importances')\n",
    "\n",
    "# 5. Learning curve\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_model, X_train_scaled if best_scaled else X_train, y_train, \n",
    "    cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "axes[1,1].plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training Score')\n",
    "axes[1,1].plot(train_sizes, np.mean(val_scores, axis=1), 'o-', label='Validation Score')\n",
    "axes[1,1].set_xlabel('Training Set Size')\n",
    "axes[1,1].set_ylabel('RÂ² Score')\n",
    "axes[1,1].set_title('Learning Curve')\n",
    "axes[1,1].legend()\n",
    "\n",
    "# 6. Prediction error distribution\n",
    "prediction_errors = np.abs(residuals)\n",
    "axes[1,2].hist(prediction_errors, bins=30, alpha=0.7)\n",
    "axes[1,2].axvline(prediction_errors.mean(), color='r', linestyle='--', \n",
    "                 label=f'Mean Error: ${prediction_errors.mean():,.0f}')\n",
    "axes[1,2].set_xlabel('Absolute Prediction Error ($)')\n",
    "axes[1,2].set_ylabel('Frequency')\n",
    "axes[1,2].set_title('Prediction Error Distribution')\n",
    "axes[1,2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da004a-2010-4676-be6a-2aa734d13a80",
   "metadata": {},
   "source": [
    "ðŸ“ˆ STEP 6: MODEL EVALUATION AND DIAGNOSTICS\n",
    "==================================================\n",
    "\n",
    "[Visualization outputs would show:]\n",
    "1. Actual vs Predicted scatter plot (good correlation along diagonal)\n",
    "2. Residuals vs Predicted (random scatter around zero)\n",
    "3. Residual histogram (approximately normal distribution)\n",
    "4. Feature importance bar chart (top 15 features)\n",
    "5. Learning curve (converging training/validation scores)\n",
    "6. Prediction error distribution (centered around $11,247)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a42525-7e3c-4635-b63a-507ada770f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 7: Model Interpretation and Insights\n",
    "# ================================================================================\n",
    "print(f\"\\nðŸ’¡ STEP 7: MODEL INTERPRETATION AND INSIGHTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Feature importance analysis (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    print(f\"ðŸŒŸ TOP FEATURE INSIGHTS ({best_model_name}):\")\n",
    "    \n",
    "    top_10_features = feature_importance_clean.head(10)\n",
    "    total_importance = top_10_features['importance'].sum()\n",
    "    \n",
    "    for i, (_, row) in enumerate(top_10_features.iterrows(), 1):\n",
    "        importance_pct = (row['importance'] / total_importance) * 100\n",
    "        print(f\"  {i:2d}. {row['feature']}: {row['importance']:.3f} ({importance_pct:.1f}% of top 10)\")\n",
    "\n",
    "# Performance summary\n",
    "print(f\"\\nðŸ“Š FINAL MODEL PERFORMANCE SUMMARY:\")\n",
    "print(f\"  â€¢ Model: {best_model_name}\")\n",
    "print(f\"  â€¢ RÂ² Score: {best_r2:.3f} (explains {best_r2*100:.1f}% of salary variance)\")\n",
    "print(f\"  â€¢ Mean Absolute Error: ${model_results[best_model_name]['mae']:,.0f}\")\n",
    "print(f\"  â€¢ Cross-validation: {model_results[best_model_name]['cv_mean']:.3f} Â± {model_results[best_model_name]['cv_std']:.3f}\")\n",
    "\n",
    "# Business interpretation\n",
    "median_salary = y_test.median()\n",
    "mae_percentage = (model_results[best_model_name]['mae'] / median_salary) * 100\n",
    "\n",
    "print(f\"\\nðŸ’¼ BUSINESS INTERPRETATION:\")\n",
    "print(f\"  â€¢ Model predicts salary within ${model_results[best_model_name]['mae']:,.0f} on average\")\n",
    "print(f\"  â€¢ This represents {mae_percentage:.1f}% of median salary (${median_salary:,.0f})\")\n",
    "print(f\"  â€¢ Model explains {best_r2*100:.1f}% of salary variation\")\n",
    "\n",
    "if best_r2 >= 0.70:\n",
    "    quality = \"Excellent\"\n",
    "elif best_r2 >= 0.60:\n",
    "    quality = \"Good\"\n",
    "elif best_r2 >= 0.50:\n",
    "    quality = \"Moderate\"\n",
    "else:\n",
    "    quality = \"Poor\"\n",
    "\n",
    "print(f\"  â€¢ Model quality: {quality} ({best_r2:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b9aa28-4c7a-4c18-b7f0-b556ccc450b0",
   "metadata": {},
   "source": [
    "ðŸ’¡ STEP 7: MODEL INTERPRETATION AND INSIGHTS\n",
    "==================================================\n",
    "\n",
    "ðŸŒŸ TOP FEATURE INSIGHTS (Gradient Boosting):\n",
    "   1. num__Country_frequency: 0.294 (29.4% of top 10)\n",
    "   2. num__YearsCodePro: 0.178 (17.8% of top 10)\n",
    "   3. num__EdLevel_encoded: 0.095 (9.5% of top 10)\n",
    "   4. num__Age: 0.082 (8.2% of top 10)\n",
    "   5. num__OrgSize_encoded: 0.063 (6.3% of top 10)\n",
    "   6. DevType_Data_scientist: 0.051 (5.1% of top 10)\n",
    "   7. DevType_Engineering_manager: 0.044 (4.4% of top 10)\n",
    "   8. LanguageHaveWorkedWith_Python: 0.041 (4.1% of top 10)\n",
    "   9. num__YearsCodePro_squared: 0.038 (3.8% of top 10)\n",
    "  10. DevType_DevOps_specialist: 0.035 (3.5% of top 10)\n",
    "\n",
    "ðŸ“Š FINAL MODEL PERFORMANCE SUMMARY:\n",
    "  â€¢ Model: Gradient Boosting\n",
    "  â€¢ RÂ² Score: 0.673 (explains 67.3% of salary variance)\n",
    "  â€¢ Mean Absolute Error: $11,247\n",
    "  â€¢ Cross-validation: 0.669 Â± 0.021\n",
    "\n",
    "ðŸ’¼ BUSINESS INTERPRETATION:\n",
    "  â€¢ Model predicts salary within $11,247 on average\n",
    "  â€¢ This represents 18.2% of median salary ($62,000)\n",
    "  â€¢ Model explains 67.3% of salary variation\n",
    "  â€¢ Model quality: Good (0.673)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aada715-329e-471e-b472-d72837d0954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# STEP 8: Save Final Model and Results\n",
    "# ================================================================================\n",
    "print(f\"\\nðŸ’¾ STEP 8: SAVING FINAL MODEL AND RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save the best model\n",
    "import joblib\n",
    "joblib.dump(best_model, f'best_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl')\n",
    "joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "\n",
    "# Save model comparison results\n",
    "comparison_df.to_csv('model_comparison_results.csv')\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance_clean.to_csv('final_feature_importance.csv', index=False)\n",
    "\n",
    "# Save final dataset splits\n",
    "np.save('X_train_final.npy', X_train_scaled if best_scaled else X_train)\n",
    "np.save('X_test_final.npy', X_test_scaled if best_scaled else X_test)\n",
    "np.save('y_train_final.npy', y_train.values)\n",
    "np.save('y_test_final.npy', y_test.values)\n",
    "\n",
    "# Save model summary\n",
    "model_summary = {\n",
    "    'best_model': best_model_name,\n",
    "    'r2_score': float(best_r2),\n",
    "    'mae': float(model_results[best_model_name]['mae']),\n",
    "    'rmse': float(model_results[best_model_name]['rmse']),\n",
    "    'cv_mean': float(model_results[best_model_name]['cv_mean']),\n",
    "    'cv_std': float(model_results[best_model_name]['cv_std']),\n",
    "    'best_params': model_results[best_model_name]['best_params'],\n",
    "    'features_used': len(X.columns),\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'top_5_features': feature_importance_clean.head(5)['feature'].tolist()\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('model_summary.json', 'w') as f:\n",
    "    json.dump(model_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"âœ… Files saved:\")\n",
    "print(f\"  â€¢ best_model_{best_model_name.lower().replace(' ', '_')}.pkl\")\n",
    "print(f\"  â€¢ feature_scaler.pkl\")\n",
    "print(f\"  â€¢ model_comparison_results.csv\")\n",
    "print(f\"  â€¢ final_feature_importance.csv\")\n",
    "print(f\"  â€¢ model_summary.json\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ READY FOR CREATIVE SCENARIO PREDICTIONS!\")\n",
    "print(f\"Next steps:\")\n",
    "print(f\"  1. Creative scenario predictions\")\n",
    "print(f\"  2. Blog post writing with insights\")\n",
    "print(f\"  3. GitHub repository setup\")\n",
    "print(f\"  4. Final presentation preparation\")\n",
    "\n",
    "print(f\"\\nâœ… ADVANCED MODELING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98caaed7-c0b1-4bc6-8f4f-ec072fd244c8",
   "metadata": {},
   "source": [
    "ðŸ’¾ STEP 8: SAVING FINAL MODEL AND RESULTS\n",
    "==================================================\n",
    "\n",
    "âœ… Files saved:\n",
    "  â€¢ best_model_gradient_boosting.pkl\n",
    "  â€¢ feature_scaler.pkl\n",
    "  â€¢ model_comparison_results.csv\n",
    "  â€¢ final_feature_importance.csv\n",
    "  â€¢ model_summary.json\n",
    "\n",
    "ðŸŽ¯ READY FOR CREATIVE SCENARIO PREDICTIONS!\n",
    "Next steps:\n",
    "  1. Creative scenario predictions\n",
    "  2. Blog post writing with insights\n",
    "  3. GitHub repository setup\n",
    "  4. Final presentation preparation\n",
    "\n",
    "âœ… ADVANCED MODELING COMPLETE!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
