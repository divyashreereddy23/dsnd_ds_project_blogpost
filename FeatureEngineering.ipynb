{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bca808b-266c-4f6c-b4e9-011d65617c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StackOverflow Survey - Advanced Feature Engineering\n",
    "# Handles complex categorical variables specific to developer survey data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27338d08-744b-4d84-888a-cffeca331ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è STACKOVERFLOW FEATURE ENGINEERING\n",
      "==================================================\n",
      "Dataset shape: (21782, 114)\n",
      "Target variable: ConvertedCompYearly\n"
     ]
    }
   ],
   "source": [
    "print(\"üõ†Ô∏è STACKOVERFLOW FEATURE ENGINEERING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load the cleaned dataset from previous analysis\n",
    "df = pd.read_csv('stackoverflow_salary_clean.csv')\n",
    "TARGET_VARIABLE = 'ConvertedCompYearly'\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Target variable: {TARGET_VARIABLE}\")\n",
    "\n",
    "# Create a copy for feature engineering\n",
    "df_features = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c2c094c-ce8a-47cb-a25c-1dd00445e4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó STEP 1: MULTI-RESPONSE CATEGORICAL ENCODING\n",
      "==================================================\n",
      "Multi-response columns found: 15\n",
      "  ‚Ä¢ DevType\n",
      "  ‚Ä¢ LanguageHaveWorkedWith\n",
      "  ‚Ä¢ LanguageWantToWorkWith\n",
      "  ‚Ä¢ DatabaseHaveWorkedWith\n",
      "  ‚Ä¢ DatabaseWantToWorkWith\n",
      "  ‚Ä¢ PlatformHaveWorkedWith\n",
      "  ‚Ä¢ PlatformWantToWorkWith\n",
      "  ‚Ä¢ WebframeHaveWorkedWith\n",
      "  ‚Ä¢ WebframeWantToWorkWith\n",
      "  ‚Ä¢ MiscTechHaveWorkedWith\n",
      "  ‚Ä¢ MiscTechWantToWorkWith\n",
      "  ‚Ä¢ ToolsTechHaveWorkedWith\n",
      "  ‚Ä¢ ToolsTechWantToWorkWith\n",
      "  ‚Ä¢ NEWCollabToolsHaveWorkedWith\n",
      "  ‚Ä¢ NEWCollabToolsWantToWorkWith\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 1: Handle Multi-Response Categorical Variables\n",
    "# ================================================================================\n",
    "print(f\"\\nüîó STEP 1: MULTI-RESPONSE CATEGORICAL ENCODING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define multi-response columns (common in StackOverflow survey)\n",
    "MULTI_RESPONSE_COLUMNS = [\n",
    "    'DevType',                    # Multiple developer types\n",
    "    'LanguageHaveWorkedWith',     # Programming languages\n",
    "    'LanguageWantToWorkWith',     # Languages want to learn\n",
    "    'DatabaseHaveWorkedWith',     # Databases used\n",
    "    'DatabaseWantToWorkWith',     # Databases want to learn\n",
    "    'PlatformHaveWorkedWith',     # Platforms used\n",
    "    'PlatformWantToWorkWith',     # Platforms want to learn\n",
    "    'WebframeHaveWorkedWith',     # Web frameworks used\n",
    "    'WebframeWantToWorkWith',     # Web frameworks want to learn\n",
    "    'MiscTechHaveWorkedWith',     # Other technologies\n",
    "    'MiscTechWantToWorkWith',     # Other technologies want to learn\n",
    "    'ToolsTechHaveWorkedWith',    # Tools and technologies\n",
    "    'ToolsTechWantToWorkWith',    # Tools want to learn\n",
    "    'NEWCollabToolsHaveWorkedWith', # Collaboration tools\n",
    "    'NEWCollabToolsWantToWorkWith', # Collaboration tools want to learn\n",
    "    'OpSysProfessional',          # Operating systems\n",
    "    'OpSysPersonal',              # Personal OS\n",
    "]\n",
    "\n",
    "# Filter to only include columns that exist in dataset\n",
    "multi_response_cols = [col for col in MULTI_RESPONSE_COLUMNS if col in df_features.columns]\n",
    "print(f\"Multi-response columns found: {len(multi_response_cols)}\")\n",
    "for col in multi_response_cols:\n",
    "    print(f\"  ‚Ä¢ {col}\")\n",
    "\n",
    "def encode_multi_response_column(df, column_name, min_frequency=50, max_features=20):\n",
    "    \"\"\"\n",
    "    Encode multi-response categorical column (semicolon-separated values)\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - column_name: Name of the column to encode\n",
    "    - min_frequency: Minimum frequency for a category to be included\n",
    "    - max_features: Maximum number of features to create\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with new binary columns\n",
    "    \"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\nüîß Encoding {column_name}:\")\n",
    "\n",
    "    # Extract all unique values\n",
    "    all_values = []\n",
    "    for responses in df[column_name].dropna():\n",
    "        if pd.notna(responses):\n",
    "            values = [val.strip() for val in str(responses).split(';')]\n",
    "            all_values.extend(values)\n",
    "\n",
    "    # Count frequencies\n",
    "    value_counts = pd.Series(all_values).value_counts()\n",
    "    \n",
    "    # Filter by minimum frequency and max features\n",
    "    top_values = value_counts[value_counts >= min_frequency].head(max_features)\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Total unique values: {len(value_counts)}\")\n",
    "    print(f\"  ‚Ä¢ Values with ‚â•{min_frequency} occurrences: {len(top_values)}\")\n",
    "    print(f\"  ‚Ä¢ Creating {len(top_values)} binary features\")\n",
    "    \n",
    "    # Create binary columns\n",
    "    encoded_df = pd.DataFrame()\n",
    "    for value in top_values.index:\n",
    "        # Clean column name\n",
    "        safe_name = re.sub(r'[^a-zA-Z0-9_]', '_', value)\n",
    "        column_name_clean = f\"{column_name}_{safe_name}\"\n",
    "        \n",
    "        # Create binary feature\n",
    "        encoded_df[column_name_clean] = df[column_name].apply(\n",
    "            lambda x: 1 if pd.notna(x) and value in str(x) else 0\n",
    "        )\n",
    "    \n",
    "    return encoded_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e12e41ac-8d04-409b-903f-46fbb14a2450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Encoding LanguageHaveWorkedWith:\n",
      "  ‚Ä¢ Total unique values: 49\n",
      "  ‚Ä¢ Values with ‚â•100 occurrences: 15\n",
      "  ‚Ä¢ Creating 15 binary features\n",
      "\n",
      "üîß Encoding DatabaseHaveWorkedWith:\n",
      "  ‚Ä¢ Total unique values: 35\n",
      "  ‚Ä¢ Values with ‚â•100 occurrences: 15\n",
      "  ‚Ä¢ Creating 15 binary features\n",
      "\n",
      "üîß Encoding WebframeHaveWorkedWith:\n",
      "  ‚Ä¢ Total unique values: 36\n",
      "  ‚Ä¢ Values with ‚â•100 occurrences: 15\n",
      "  ‚Ä¢ Creating 15 binary features\n",
      "\n",
      "üîß Encoding PlatformHaveWorkedWith:\n",
      "  ‚Ä¢ Total unique values: 27\n",
      "  ‚Ä¢ Values with ‚â•100 occurrences: 15\n",
      "  ‚Ä¢ Creating 15 binary features\n",
      "\n",
      "üîß Encoding DevType:\n",
      "  ‚Ä¢ Total unique values: 34\n",
      "  ‚Ä¢ Values with ‚â•100 occurrences: 15\n",
      "  ‚Ä¢ Creating 15 binary features\n"
     ]
    }
   ],
   "source": [
    "# Apply multi-response encoding to key technology columns\n",
    "tech_columns_to_encode = [\n",
    "    'LanguageHaveWorkedWith',\n",
    "    'DatabaseHaveWorkedWith', \n",
    "    'WebframeHaveWorkedWith',\n",
    "    'PlatformHaveWorkedWith',\n",
    "    'DevType'\n",
    "]\n",
    "\n",
    "for col in tech_columns_to_encode:\n",
    "    if col in df_features.columns:\n",
    "        encoded_features = encode_multi_response_column(df_features, col, min_frequency=100, max_features=15)\n",
    "        df_features = pd.concat([df_features, encoded_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a72a96de-e89d-4036-a475-9c69b2d52208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù STEP 2: SIMPLE CATEGORICAL ENCODING\n",
      "==================================================\n",
      "Simple categorical columns found: 6\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 2: Handle Simple Categorical Variables\n",
    "# ================================================================================\n",
    "print(f\"\\nüìù STEP 2: SIMPLE CATEGORICAL ENCODING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define simple categorical columns and their encoding strategy\n",
    "SIMPLE_CATEGORICAL_COLUMNS = {\n",
    "    'Country': 'frequency',           # High cardinality - use frequency encoding\n",
    "    'EdLevel': 'ordinal',            # Ordinal - use custom ordinal encoding\n",
    "    'Employment': 'onehot',          # Low cardinality - one-hot encode\n",
    "    'CompanyType': 'onehot',         # Low cardinality - one-hot encode\n",
    "    'OrgSize': 'ordinal',            # Ordinal - use custom ordinal encoding\n",
    "    'WorkRemote': 'onehot',          # Low cardinality - one-hot encode\n",
    "    'MainBranch': 'onehot',          # Low cardinality - one-hot encode\n",
    "    'Gender': 'onehot',              # Low cardinality - one-hot encode\n",
    "    'JobSat': 'ordinal',             # Ordinal satisfaction levels\n",
    "    'CareerSat': 'ordinal',          # Ordinal satisfaction levels\n",
    "    'MentalHealth': 'onehot',        # Low cardinality - one-hot encode\n",
    "    'Ethnicity': 'frequency',        # High cardinality - use frequency encoding\n",
    "    'OpSysProfessional': 'onehot',   # Low cardinality - one-hot encode\n",
    "    'OpSysPersonal': 'onehot',       # Low cardinality - one-hot encode\n",
    "}\n",
    "\n",
    "# Filter to only include columns that exist\n",
    "simple_categorical_cols = {k: v for k, v in SIMPLE_CATEGORICAL_COLUMNS.items() if k in df_features.columns}\n",
    "print(f\"Simple categorical columns found: {len(simple_categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2eb5d0d7-21c4-4ec7-bd7c-72b50c763f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_ordinal_column(df, column_name, custom_order=None):\n",
    "    \"\"\"\n",
    "    Encode ordinal categorical column with proper ordering\n",
    "    \"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        return pd.Series()\n",
    "    \n",
    "    print(f\"  üî¢ Ordinal encoding: {column_name}\")\n",
    "    \n",
    "    # Define custom orderings for common StackOverflow columns\n",
    "    ordinal_mappings = {\n",
    "        'EdLevel': {\n",
    "            'Primary/elementary school': 1,\n",
    "            'Secondary school': 2,\n",
    "            'Some college/university study without earning a degree': 3,\n",
    "            'Associate degree': 4,\n",
    "            \"Bachelor's degree\": 5,\n",
    "            \"Master's degree\": 6,\n",
    "            'Professional degree': 7,\n",
    "            'Doctorate': 8\n",
    "        },\n",
    "        'OrgSize': {\n",
    "            'Just me - I am a freelancer, sole proprietor, etc.': 1,\n",
    "            '2-9 employees': 2,\n",
    "            '10-19 employees': 3,\n",
    "            '20-99 employees': 4,\n",
    "            '100-499 employees': 5,\n",
    "            '500-999 employees': 6,\n",
    "            '1,000-4,999 employees': 7,\n",
    "            '5,000-9,999 employees': 8,\n",
    "            '10,000 or more employees': 9\n",
    "        },\n",
    "        'JobSat': {\n",
    "            'Very dissatisfied': 1,\n",
    "            'Slightly dissatisfied': 2,\n",
    "            'Neither satisfied nor dissatisfied': 3,\n",
    "            'Slightly satisfied': 4,\n",
    "            'Very satisfied': 5\n",
    "        },\n",
    "        'CareerSat': {\n",
    "            'Very dissatisfied': 1,\n",
    "            'Slightly dissatisfied': 2,\n",
    "            'Neither satisfied nor dissatisfied': 3,\n",
    "            'Slightly satisfied': 4,\n",
    "            'Very satisfied': 5\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if custom_order:\n",
    "        mapping = custom_order\n",
    "    elif column_name in ordinal_mappings:\n",
    "        mapping = ordinal_mappings[column_name]\n",
    "    else:\n",
    "        # Default: use alphabetical order\n",
    "        unique_values = sorted(df[column_name].dropna().unique())\n",
    "        mapping = {val: i+1 for i, val in enumerate(unique_values)}\n",
    "    \n",
    "    print(f\"    ‚Ä¢ Mapping: {mapping}\")\n",
    "    encoded_series = df[column_name].map(mapping)\n",
    "    \n",
    "    return encoded_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2924a3c-afaf-4e27-b8ef-3ec9a53fd1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_frequency_column(df, column_name):\n",
    "    \"\"\"\n",
    "    Encode high-cardinality categorical column using frequency encoding\n",
    "    \"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        return pd.Series()\n",
    "    \n",
    "    print(f\"  üìä Frequency encoding: {column_name}\")\n",
    "    \n",
    "    # Calculate frequency of each category\n",
    "    frequency_map = df[column_name].value_counts().to_dict()\n",
    "    encoded_series = df[column_name].map(frequency_map)\n",
    "    \n",
    "    print(f\"    ‚Ä¢ Unique categories: {len(frequency_map)}\")\n",
    "    print(f\"    ‚Ä¢ Top 5 categories: {dict(list(frequency_map.items())[:5])}\")\n",
    "    \n",
    "    return encoded_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad7bcfea-0f1c-4d44-93b8-47e1b3c37f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot_column(df, column_name, max_categories=10):\n",
    "    \"\"\"\n",
    "    One-hot encode categorical column\n",
    "    \"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"  üéØ One-hot encoding: {column_name}\")\n",
    "    \n",
    "    # Limit categories to prevent too many features\n",
    "    value_counts = df[column_name].value_counts()\n",
    "    if len(value_counts) > max_categories:\n",
    "        top_categories = value_counts.head(max_categories).index\n",
    "        df_limited = df[column_name].apply(\n",
    "            lambda x: x if x in top_categories else 'Other'\n",
    "        )\n",
    "        print(f\"    ‚Ä¢ Limited to top {max_categories} categories (+ Other)\")\n",
    "    else:\n",
    "        df_limited = df[column_name]\n",
    "    \n",
    "    # Create dummy variables\n",
    "    dummy_df = pd.get_dummies(df_limited, prefix=column_name, drop_first=True)\n",
    "    print(f\"    ‚Ä¢ Created {dummy_df.shape[1]} binary features\")\n",
    "    \n",
    "    return dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19e3cf22-6352-4df5-adf3-d5087182bfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Processing Country (strategy: frequency):\n",
      "  üìä Frequency encoding: Country\n",
      "    ‚Ä¢ Unique categories: 156\n",
      "    ‚Ä¢ Top 5 categories: {'United States of America': 4590, 'Germany': 2020, 'United Kingdom of Great Britain and Northern Ireland': 1375, 'Ukraine': 1044, 'France': 908}\n",
      "\n",
      "üîß Processing EdLevel (strategy: ordinal):\n",
      "  üî¢ Ordinal encoding: EdLevel\n",
      "    ‚Ä¢ Mapping: {'Primary/elementary school': 1, 'Secondary school': 2, 'Some college/university study without earning a degree': 3, 'Associate degree': 4, \"Bachelor's degree\": 5, \"Master's degree\": 6, 'Professional degree': 7, 'Doctorate': 8}\n",
      "\n",
      "üîß Processing Employment (strategy: onehot):\n",
      "  üéØ One-hot encoding: Employment\n",
      "    ‚Ä¢ Limited to top 10 categories (+ Other)\n",
      "    ‚Ä¢ Created 10 binary features\n",
      "\n",
      "üîß Processing OrgSize (strategy: ordinal):\n",
      "  üî¢ Ordinal encoding: OrgSize\n",
      "    ‚Ä¢ Mapping: {'Just me - I am a freelancer, sole proprietor, etc.': 1, '2-9 employees': 2, '10-19 employees': 3, '20-99 employees': 4, '100-499 employees': 5, '500-999 employees': 6, '1,000-4,999 employees': 7, '5,000-9,999 employees': 8, '10,000 or more employees': 9}\n",
      "\n",
      "üîß Processing MainBranch (strategy: onehot):\n",
      "  üéØ One-hot encoding: MainBranch\n",
      "    ‚Ä¢ Created 1 binary features\n",
      "\n",
      "üîß Processing JobSat (strategy: ordinal):\n",
      "  üî¢ Ordinal encoding: JobSat\n",
      "    ‚Ä¢ Mapping: {'Very dissatisfied': 1, 'Slightly dissatisfied': 2, 'Neither satisfied nor dissatisfied': 3, 'Slightly satisfied': 4, 'Very satisfied': 5}\n"
     ]
    }
   ],
   "source": [
    "# Apply encoding strategies\n",
    "for col_name, strategy in simple_categorical_cols.items():\n",
    "    print(f\"\\nüîß Processing {col_name} (strategy: {strategy}):\")\n",
    "    \n",
    "    if strategy == 'ordinal':\n",
    "        encoded_series = encode_ordinal_column(df_features, col_name)\n",
    "        if not encoded_series.empty:\n",
    "            df_features[f'{col_name}_encoded'] = encoded_series\n",
    "    \n",
    "    elif strategy == 'frequency':\n",
    "        encoded_series = encode_frequency_column(df_features, col_name)\n",
    "        if not encoded_series.empty:\n",
    "            df_features[f'{col_name}_frequency'] = encoded_series\n",
    "    \n",
    "    elif strategy == 'onehot':\n",
    "        encoded_df = encode_onehot_column(df_features, col_name)\n",
    "        if not encoded_df.empty:\n",
    "            df_features = pd.concat([df_features, encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d1d148d-225d-47f9-b60e-db5caa460454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä STEP 3: NUMERICAL FEATURE ENGINEERING\n",
      "==================================================\n",
      "Numerical columns: 92\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 3: Handle Numerical Variables and Create Interactions\n",
    "# ================================================================================\n",
    "print(f\"\\nüìä STEP 3: NUMERICAL FEATURE ENGINEERING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_cols = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_cols = [col for col in numerical_cols if col != TARGET_VARIABLE]\n",
    "\n",
    "print(f\"Numerical columns: {len(numerical_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "570a6ec1-d888-4610-99d3-335fc99c2383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Engineering YearsCodePro:\n",
      "  ‚Ä¢ Created experience categories\n",
      "  ‚Ä¢ Created experience squared feature\n",
      "\n",
      "üîß Engineering Age:\n",
      "  ‚Ä¢ Created age categories\n"
     ]
    }
   ],
   "source": [
    "# Handle specific numerical columns with domain knowledge\n",
    "if 'YearsCodePro' in df_features.columns:\n",
    "    print(f\"\\nüîß Engineering YearsCodePro:\")\n",
    "\n",
    "    # Convert to numeric, coerce errors to NaN\n",
    "    df_features['YearsCodePro'] = pd.to_numeric(df_features['YearsCodePro'], errors='coerce')\n",
    "    \n",
    "    # Create experience categories\n",
    "    df_features['Experience_Category'] = pd.cut(\n",
    "        df_features['YearsCodePro'], \n",
    "        bins=[0, 2, 5, 10, 20, 50], \n",
    "        labels=['Junior', 'Mid', 'Senior', 'Lead', 'Expert']\n",
    "    )\n",
    "    \n",
    "    # Create experience squared (for non-linear relationships)\n",
    "    df_features['YearsCodePro_squared'] = df_features['YearsCodePro'] ** 2\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Created experience categories\")\n",
    "    print(f\"  ‚Ä¢ Created experience squared feature\")\n",
    "\n",
    "if 'Age' in df_features.columns:\n",
    "    print(f\"\\nüîß Engineering Age:\")\n",
    "    \n",
    "    # Convert to numeric, coerce errors to NaN\n",
    "    df_features['Age'] = pd.to_numeric(df_features['Age'], errors='coerce')\n",
    "    \n",
    "    # Create age categories\n",
    "    df_features['Age_Category'] = pd.cut(\n",
    "        df_features['Age'], \n",
    "        bins=[0, 25, 35, 45, 55, 100], \n",
    "        labels=['Young', 'Early_Career', 'Mid_Career', 'Senior_Career', 'Veteran']\n",
    "    )\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Created age categories\")\n",
    "\n",
    "if 'WorkWeekHrs' in df_features.columns:\n",
    "    print(f\"\\nüîß Engineering WorkWeekHrs:\")\n",
    "    \n",
    "    # Convert to numeric, coerce errors to NaN\n",
    "    df_features['WorkWeekHrs'] = pd.to_numeric(df_features['WorkWeekHrs'], errors='coerce')\n",
    "    \n",
    "    # Create work-life balance categories\n",
    "    df_features['Work_Life_Balance'] = pd.cut(\n",
    "        df_features['WorkWeekHrs'], \n",
    "        bins=[0, 35, 40, 45, 50, 100], \n",
    "        labels=['Part_Time', 'Standard', 'Moderate_Overtime', 'High_Overtime', 'Extreme_Overtime']\n",
    "    )\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Created work-life balance categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d849eb10-ee93-41ae-8eb7-5ca167ccbf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Creating interaction features:\n",
      "  ‚Ä¢ Created YearsCodePro √ó EdLevel_encoded\n",
      "  ‚Ä¢ Created YearsCodePro √ó OrgSize_encoded\n",
      "  ‚Ä¢ Created Age √ó YearsCodePro\n"
     ]
    }
   ],
   "source": [
    "# Create interaction features between important variables\n",
    "print(f\"\\nüîó Creating interaction features:\")\n",
    "\n",
    "interaction_pairs = [\n",
    "    ('YearsCodePro', 'EdLevel_encoded'),\n",
    "    ('YearsCodePro', 'OrgSize_encoded'),\n",
    "    ('Age', 'YearsCodePro'),\n",
    "]\n",
    "\n",
    "for col1, col2 in interaction_pairs:\n",
    "    if col1 in df_features.columns and col2 in df_features.columns:\n",
    "        # Multiplicative interaction\n",
    "        df_features[f'{col1}_x_{col2}'] = df_features[col1] * df_features[col2]\n",
    "        print(f\"  ‚Ä¢ Created {col1} √ó {col2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57041046-356f-4fea-bf0f-523e221bb207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ STEP 4: FEATURE SELECTION AND FINAL PREPARATION\n",
      "==================================================\n",
      "Removing 21 original categorical columns\n",
      "Dataset shape after feature engineering: (21782, 189)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 4: Feature Selection and Final Preparation\n",
    "# ================================================================================\n",
    "print(f\"\\nüéØ STEP 4: FEATURE SELECTION AND FINAL PREPARATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Remove original categorical columns that were encoded\n",
    "original_categorical_cols = list(simple_categorical_cols.keys()) + multi_response_cols\n",
    "original_categorical_cols = [col for col in original_categorical_cols if col in df_features.columns]\n",
    "\n",
    "print(f\"Removing {len(original_categorical_cols)} original categorical columns\")\n",
    "\n",
    "# Keep the target variable and remove original categoricals\n",
    "columns_to_keep = [col for col in df_features.columns if col not in original_categorical_cols or col == TARGET_VARIABLE]\n",
    "df_final = df_features[columns_to_keep].copy()\n",
    "\n",
    "print(f\"Dataset shape after feature engineering: {df_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25b47fcb-84b8-4a12-b66c-712b6d436a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Handling missing values:\n",
      "Columns with missing values: 6\n",
      "‚úÖ Missing values handled\n"
     ]
    }
   ],
   "source": [
    "# Handle any remaining missing values\n",
    "print(f\"\\nHandling missing values:\")\n",
    "missing_counts = df_final.isnull().sum()\n",
    "columns_with_missing = missing_counts[missing_counts > 0]\n",
    "\n",
    "if len(columns_with_missing) > 0:\n",
    "    print(f\"Columns with missing values: {len(columns_with_missing)}\")\n",
    "    \n",
    "    for col in columns_with_missing.index:\n",
    "        if df_final[col].dtype in ['float64', 'int64']:\n",
    "            # Fill numerical columns with median\n",
    "            df_final[col] = df_final[col].fillna(df_final[col].median())\n",
    "        else:\n",
    "            # Check if column is categorical\n",
    "            if pd.api.types.is_categorical_dtype(df_final[col]):\n",
    "                # Add 'Unknown' to the categories first if using it\n",
    "                if len(df_final[col].mode()) > 0:\n",
    "                    fill_value = df_final[col].mode().iloc[0]\n",
    "                    df_final[col] = df_final[col].fillna(fill_value)\n",
    "                else:\n",
    "                    # Add 'Unknown' to categories before filling\n",
    "                    df_final[col] = df_final[col].cat.add_categories(['Unknown'])\n",
    "                    df_final[col] = df_final[col].fillna('Unknown')\n",
    "            else:\n",
    "                # For non-categorical columns\n",
    "                df_final[col] = df_final[col].fillna(df_final[col].mode().iloc[0] if len(df_final[col].mode()) > 0 else 'Unknown')\n",
    "    \n",
    "    print(f\"‚úÖ Missing values handled\")\n",
    "else:\n",
    "    print(f\"‚úÖ No missing values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32e75e74-7169-46d9-a1bb-dbce3e38f215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (21782, 188), y shape: (21782,)\n",
      "\n",
      "Top 20 most important features:\n",
      "                                      feature  importance\n",
      "2                              num__CompTotal    0.467174\n",
      "88                     num__Country_frequency    0.315317\n",
      "1                           num__YearsCodePro    0.031459\n",
      "91                  num__YearsCodePro_squared    0.030669\n",
      "6358        cat__Currency_BRL\\tBrazilian real    0.018735\n",
      "6376          cat__Currency_EUR European Euro    0.013996\n",
      "6387    cat__Currency_ILS\\tIsraeli new shekel    0.010748\n",
      "6447  cat__Currency_USD\\tUnited States dollar    0.009626\n",
      "6363           cat__Currency_CHF\\tSwiss franc    0.007960\n",
      "6362       cat__Currency_CAD\\tCanadian dollar    0.006307\n",
      "6351     cat__Currency_AUD\\tAustralian dollar    0.006269\n",
      "6378        cat__Currency_GBP\\tPound sterling    0.005770\n",
      "6426          cat__Currency_PLN\\tPolish zloty    0.004440\n",
      "6371          cat__Currency_DKK\\tDanish krone    0.004338\n",
      "6435      cat__Currency_SGD\\tSingapore dollar    0.004015\n",
      "6445     cat__Currency_UAH\\tUkrainian hryvnia    0.002923\n",
      "6434         cat__Currency_SEK\\tSwedish krona    0.002875\n",
      "6420       cat__Currency_NOK\\tNorwegian krone    0.002856\n",
      "6422    cat__Currency_NZD\\tNew Zealand dollar    0.001992\n",
      "6389          cat__Currency_INR\\tIndian rupee    0.001641\n",
      "\n",
      "Baseline Random Forest performance:\n",
      "  ‚Ä¢ R¬≤ Score: 0.952\n",
      "  ‚Ä¢ Mean Absolute Error: $2,947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder  # Fixed the import statement\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer  # Added imputer for handling missing values\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import pandas as pd\n",
    "\n",
    "# First, check that X and y have the same number of rows\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# Make sure categorical_cols and numeric_cols are defined correctly\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Create preprocessing pipeline with imputation steps\n",
    "# Remove the 'passthrough' step which is causing issues\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), numeric_cols),  # Simplified numeric pipeline\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create pipeline with preprocessing and model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# The beginning of your code remains the same until after pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "model = pipeline.named_steps['model']\n",
    "preprocessor = pipeline.named_steps['preprocessor']\n",
    "\n",
    "# Get feature names after transformation - CORRECTED APPROACH\n",
    "# Use get_feature_names_out() method from the entire preprocessor\n",
    "try:\n",
    "    # For scikit-learn >= 1.0\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "except AttributeError:\n",
    "    # For older scikit-learn versions\n",
    "    # This is a more complex fallback approach\n",
    "    feature_names = []\n",
    "    \n",
    "    # Add numeric feature names\n",
    "    if len(numeric_cols) > 0:\n",
    "        feature_names.extend(numeric_cols)\n",
    "    \n",
    "    # Add transformed categorical feature names\n",
    "    if len(categorical_cols) > 0:\n",
    "        ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "        cat_feature_names = ohe.get_feature_names_out(categorical_cols)\n",
    "        feature_names.extend(cat_feature_names)\n",
    "\n",
    "# Ensure feature_names length matches feature_importances_ length\n",
    "if len(feature_names) != len(model.feature_importances_):\n",
    "    print(f\"Warning: Feature names length ({len(feature_names)}) doesn't match \"\n",
    "          f\"feature importances length ({len(model.feature_importances_)})\")\n",
    "    # Use generic feature names as fallback\n",
    "    feature_names = [f'feature_{i}' for i in range(len(model.feature_importances_))]\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 20 most important features:\")\n",
    "print(feature_importance.head(20))\n",
    "\n",
    "# Basic model performance\n",
    "y_pred = pipeline.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nBaseline Random Forest performance:\")\n",
    "print(f\"  ‚Ä¢ R¬≤ Score: {r2:.3f}\")\n",
    "print(f\"  ‚Ä¢ Mean Absolute Error: ${mae:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e493a071-a270-46c3-abfe-71b78d2648d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ STEP 6: SAVING PROCESSED DATA\n",
      "==================================================\n",
      "‚úÖ Feature engineered dataset saved: 'stackoverflow_features_engineered.csv'\n",
      "‚úÖ Feature importance saved: 'feature_importance.csv'\n",
      "‚úÖ Feature engineering summary saved: 'feature_engineering_summary.json'\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 6: Save Processed Data\n",
    "# ================================================================================\n",
    "print(f\"\\nüíæ STEP 6: SAVING PROCESSED DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save the final dataset\n",
    "df_final.to_csv('stackoverflow_features_engineered.csv', index=False)\n",
    "print(f\"‚úÖ Feature engineered dataset saved: 'stackoverflow_features_engineered.csv'\")\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('feature_importance.csv', index=False)\n",
    "print(f\"‚úÖ Feature importance saved: 'feature_importance.csv'\")\n",
    "\n",
    "# Save feature engineering summary\n",
    "feature_summary = {\n",
    "    'original_shape': df.shape,\n",
    "    'final_shape': df_final.shape,\n",
    "    'features_added': df_final.shape[1] - df.shape[1],\n",
    "    'multi_response_columns_processed': len([col for col in multi_response_cols if col in df.columns]),\n",
    "    'simple_categorical_columns_processed': len(simple_categorical_cols),\n",
    "    'top_features': feature_importance.head(10)['feature'].tolist(),\n",
    "    'baseline_r2_score': r2,\n",
    "    'baseline_mae': mae\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('feature_engineering_summary.json', 'w') as f:\n",
    "    json.dump(feature_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"‚úÖ Feature engineering summary saved: 'feature_engineering_summary.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a9f846d-49d2-4941-8cc5-9b073905fba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã FEATURE ENGINEERING REPORT\n",
      "==================================================\n",
      "üéØ TRANSFORMATION SUMMARY:\n",
      "  ‚Ä¢ Original dataset: 21,782 rows √ó 114 columns\n",
      "  ‚Ä¢ Final dataset: 21,782 rows √ó 189 columns\n",
      "  ‚Ä¢ Features added: 75\n",
      "  ‚Ä¢ Multi-response columns processed: 15\n",
      "  ‚Ä¢ Simple categorical columns processed: 6\n",
      "\n",
      "üèÜ TOP 10 MOST IMPORTANT FEATURES:\n",
      "   1. num__CompTotal: 0.467\n",
      "   2. num__Country_frequency: 0.315\n",
      "   3. num__YearsCodePro: 0.031\n",
      "   4. num__YearsCodePro_squared: 0.031\n",
      "   5. cat__Currency_BRL\tBrazilian real: 0.019\n",
      "   6. cat__Currency_EUR European Euro: 0.014\n",
      "   7. cat__Currency_ILS\tIsraeli new shekel: 0.011\n",
      "   8. cat__Currency_USD\tUnited States dollar: 0.010\n",
      "   9. cat__Currency_CHF\tSwiss franc: 0.008\n",
      "  10. cat__Currency_CAD\tCanadian dollar: 0.006\n",
      "\n",
      "üìä BASELINE MODEL PERFORMANCE:\n",
      "  ‚Ä¢ R¬≤ Score: 0.952 (explains 95.2% of salary variance)\n",
      "  ‚Ä¢ Mean Absolute Error: $2,947\n",
      "  ‚Ä¢ Model predicts salary within $2,947 on average\n",
      "\n",
      "üöÄ READY FOR ADVANCED MODELING!\n",
      "Next steps:\n",
      "  1. Advanced model training (Random Forest, Gradient Boosting, etc.)\n",
      "  2. Hyperparameter tuning\n",
      "  3. Model evaluation and comparison\n",
      "  4. Creative scenario predictions\n",
      "  5. Blog post writing with insights\n",
      "\n",
      "‚úÖ FEATURE ENGINEERING COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# STEP 7: Feature Engineering Report\n",
    "# ================================================================================\n",
    "print(f\"\\nüìã FEATURE ENGINEERING REPORT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"üéØ TRANSFORMATION SUMMARY:\")\n",
    "print(f\"  ‚Ä¢ Original dataset: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"  ‚Ä¢ Final dataset: {df_final.shape[0]:,} rows √ó {df_final.shape[1]} columns\")\n",
    "print(f\"  ‚Ä¢ Features added: {df_final.shape[1] - df.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Multi-response columns processed: {len([col for col in multi_response_cols if col in df.columns])}\")\n",
    "print(f\"  ‚Ä¢ Simple categorical columns processed: {len(simple_categorical_cols)}\")\n",
    "\n",
    "print(f\"\\nüèÜ TOP 10 MOST IMPORTANT FEATURES:\")\n",
    "for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):\n",
    "    print(f\"  {i:2d}. {row['feature']}: {row['importance']:.3f}\")\n",
    "\n",
    "print(f\"\\nüìä BASELINE MODEL PERFORMANCE:\")\n",
    "print(f\"  ‚Ä¢ R¬≤ Score: {r2:.3f} (explains {r2*100:.1f}% of salary variance)\")\n",
    "print(f\"  ‚Ä¢ Mean Absolute Error: ${mae:,.0f}\")\n",
    "print(f\"  ‚Ä¢ Model predicts salary within ${mae:,.0f} on average\")\n",
    "\n",
    "print(f\"\\nüöÄ READY FOR ADVANCED MODELING!\")\n",
    "print(f\"Next steps:\")\n",
    "print(f\"  1. Advanced model training (Random Forest, Gradient Boosting, etc.)\")\n",
    "print(f\"  2. Hyperparameter tuning\")\n",
    "print(f\"  3. Model evaluation and comparison\")\n",
    "print(f\"  4. Creative scenario predictions\")\n",
    "print(f\"  5. Blog post writing with insights\")\n",
    "\n",
    "print(f\"\\n‚úÖ FEATURE ENGINEERING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5912181c-9836-42d7-b6ef-dfbc81940aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
